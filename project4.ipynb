{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleveland.csv')\n",
    "\n",
    "# Rename 'num' column to 'disease' and change 1,2,3,4 to 1\n",
    "df = df.rename({'num':'disease'}, axis=1)\n",
    "df['disease'] = df.disease.apply(lambda x: min(x, 1))\n",
    "\n",
    "# get rid of missing data\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding good features\n",
    "\n",
    "here, we categorize the different kinds of data that we have, and then compare the distribution of that feature in both populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = [\n",
    "    'age',\n",
    "    'trestbps',\n",
    "    'chol',\n",
    "    'thalach',\n",
    "    'oldpeak',\n",
    "]\n",
    "\n",
    "categorical = [\n",
    "    'sex',\n",
    "    'cp',\n",
    "    'fbs',\n",
    "    'restecg',\n",
    "    'exang',\n",
    "    'slope',\n",
    "    'ca',\n",
    "    'thal',\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, figsize=(20, 5))\n",
    "fig.suptitle('Numeric Features by presense of disease')\n",
    "for i in range(len(numeric)):\n",
    "    sns.kdeplot(data=df, x=numeric[i], hue='disease', ax=axs[i], fill=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
    "fig.suptitle('Categorical Features by presense of disease')\n",
    "for i in range(len(categorical)):\n",
    "    sns.countplot(data=df, x=categorical[i], hue='disease', ax=axs[i//4][i%4])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those graphs, there are some suspicious features. specifically, I will investigate if there is a correlation between disease and:\n",
    "1. age\n",
    "2. thalach\n",
    "3. oldpeak\n",
    "4. sex\n",
    "5. cp\n",
    "6. slope\n",
    "7. ca\n",
    "8. thal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if there is a difference in the means between the two groups for each numeric feature\n",
    "print('t-test for age')\n",
    "print(ttest_ind(df[df.disease == 0].age, df[df.disease == 1].age))\n",
    "print('t-test for thalach')\n",
    "print(ttest_ind(df[df.disease == 0].thalach, df[df.disease == 1].thalach))\n",
    "print('t-test for oldpeak')\n",
    "print(ttest_ind(df[df.disease == 0].oldpeak, df[df.disease == 1].oldpeak))\n",
    "\n",
    "# test if there is a difference in the distribution of the categorical features between the two groups\n",
    "print('chi2 test for sex')\n",
    "print(chi2_contingency(pd.crosstab(df.sex, df.disease)))\n",
    "print('chi2 test for cp')\n",
    "print(chi2_contingency(pd.crosstab(df.cp, df.disease)))\n",
    "print('chi2 test for slope')\n",
    "print(chi2_contingency(pd.crosstab(df.slope, df.disease)))\n",
    "print('chi2 test for ca')\n",
    "print(chi2_contingency(pd.crosstab(df.ca, df.disease)))\n",
    "print('chi2 test for thal')\n",
    "print(chi2_contingency(pd.crosstab(df.thal, df.disease)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that it looks like all of the selected attributes are extremely likely to be correlated with the presence of heart dis|ease.\n",
    "\n",
    "We will select all of these features for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'age_s',\n",
    "    'thalach_s',\n",
    "    'oldpeak_s',\n",
    "    'sex',\n",
    "    'cp',\n",
    "    'slope',\n",
    "    'ca',\n",
    "    'thal',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Attributes\n",
    "\n",
    "In this case, the categorical features will not be standardized in any way. This is fine because all of the selected categorical features are either binary or ordinal, and thus the numeric representations make sense as euclidean dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standardize numeric features\n",
    "df['age_s'] = (df.age - df.age.mean())/df.age.std()\n",
    "df['thalach_s'] = (df.thalach - df.thalach.mean())/df.thalach.std()\n",
    "df['oldpeak_s'] = (df.oldpeak - df.oldpeak.mean())/df.oldpeak.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K optimization\n",
    "\n",
    "We will now use k-fold cross-validation to test the performance of the model with k values ranging from 1 to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# Create a function for prediction and evaluation\n",
    "def predict_and_evaluate(X, y, k_values, n_splits=10):\n",
    "    results = []\n",
    "\n",
    "    # Perform K-Fold cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    for k in k_values:\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Create nearest neighbors object\n",
    "            nn = NearestNeighbors(n_neighbors=k, metric='euclidean', algorithm='auto')\n",
    "            nn.fit(X_train)\n",
    "\n",
    "            # Find the k nearest neighbors to the test set\n",
    "            distances, indices = nn.kneighbors(X_test)\n",
    "\n",
    "            for i in range(len(X_test)):\n",
    "                nbrs_diseased = y_train[indices[i]].flatten()\n",
    "                predict = pd.Series(nbrs_diseased).mode()[0]  # Most common label\n",
    "                y_pred.append(predict)\n",
    "                y_true.append(y_test[i][0])\n",
    "\n",
    "        # Calculate precision, recall, and F1 scores\n",
    "        (p, r, f, s) = precision_recall_fscore_support(y_true, y_pred, labels=[1])\n",
    "        results.append((k, p, r, f, s))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main execution\n",
    "X = df[selected_features].values\n",
    "y = df[['disease']].values\n",
    "\n",
    "\n",
    "# Define the range of k values to try\n",
    "k_values = range(1, 201)  # Test k from 1 to 120\n",
    "results = predict_and_evaluate(X, y, k_values)\n",
    "\n",
    "# # Print the results\n",
    "# for k, p, r, f, s in results:\n",
    "#     print(f'k={k}, precision={p}, recall={r}, f-score={f}, support={s}')\n",
    "\n",
    "plt.plot([x[0] for x in results], [x[3] for x in results])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('F1 Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1-score appears to be the highest when k is near 10, so we will select that as our k value. Interestingly, we see two maxima for this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Challenge Dataset (replace with name of test file)\n",
    "df = pd.read_csv('cleveland-test-sample.csv')\n",
    "\n",
    "# Rename 'num' column to 'disease' and change 1,2,3,4 to 1\n",
    "df = df.rename({'num':'disease'}, axis=1)\n",
    "df['disease'] = df.disease.apply(lambda x: min(x, 1))\n",
    "\n",
    "# get rid of missing data\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# standardize numeric features\n",
    "df['age_s'] = (df.age - df.age.mean())/df.age.std()\n",
    "df['thalach_s'] = (df.thalach - df.thalach.mean())/df.thalach.std()\n",
    "df['oldpeak_s'] = (df.oldpeak - df.oldpeak.mean())/df.oldpeak.std()\n",
    "\n",
    "X = df[selected_features].values\n",
    "y = df[['disease']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, metric='euclidean', algorithm='auto')\n",
    "\n",
    "nn.fit(X)\n",
    "\n",
    "distances, indices = nn.kneighbors(X)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for i in range(len(X)):\n",
    "    nbrs_diseased = y[indices[i]].flatten()\n",
    "    predict = pd.Series(nbrs_diseased).mode()[0]  # Most common label\n",
    "    y_pred.append(predict)\n",
    "    y_true.append(y[i][0])\n",
    "\n",
    "(p, r, f, s) = precision_recall_fscore_support(y_true, y_pred, labels=[1])\n",
    "print(f'precision={p}, recall={r}, f-score={f}, support={s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "In this part, we will analyze a breast cancer database from wisconson. The \"diagnosis\" label indicates if the tumor was malignant (\"M\") or benign (\"B\"). The rest of the features are extracted from a digitized image of a fine needle aspirate of a breast mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wdbc.csv')\n",
    "\n",
    "# rename diagnosis to positive, and change M to 1 and B to 0\n",
    "df = df.rename({'diagnosis':'positive'}, axis=1)\n",
    "df['positive'] = df.positive.apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "# drop the id column\n",
    "df.drop('id', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find good features\n",
    "\n",
    "In this case, all of the features are continuous and numeric, so we can use the same tools for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[1:]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, nrows=6, figsize=(20, 20))\n",
    "for i in range(len(features)):\n",
    "    sns.kdeplot(data=df, x=features[i], hue='positive', ax=axs[i//5][i%5], fill=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further investigate the following promising features:\n",
    "1. radius1\n",
    "2. perimeter1\n",
    "3. area1\n",
    "4. concavity1\n",
    "5. concave_points1\n",
    "6. radius2\n",
    "7. perimeter2\n",
    "8. area2\n",
    "9. radius3\n",
    "10. perimeter3\n",
    "11. area3\n",
    "12. concavity3\n",
    "13. concave_points3\n",
    "\n",
    "However, that's a lot of dimensions for a KNN model, so we will limit ourselves to the best options. We can score the options by their t-statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_round1 = [\n",
    "    'radius1',\n",
    "    'perimeter1',\n",
    "    'area1',\n",
    "    'concavity1',\n",
    "    'concave_points1',\n",
    "    'radius2',\n",
    "    'perimeter2',\n",
    "    'area2',\n",
    "    'radius3',\n",
    "    'perimeter3',\n",
    "    'area3',\n",
    "    'concavity3',\n",
    "    'concave_points3',\n",
    "]\n",
    "\n",
    "ttests = {}\n",
    "\n",
    "for feature in selected_round1:\n",
    "    ttests[feature] = ttest_ind(df[df.positive == 0][feature], df[df.positive == 1][feature])\n",
    "\n",
    "# sort by magnitude of statistic\n",
    "results = sorted(ttests.items(), key=lambda x: abs(x[1].statistic), reverse=True)\n",
    "\n",
    "pd.DataFrame(results, columns=['feature', 'ttest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a ordered list of our selected features in order of best to worst, we can do another kind of optimization! Starting with one parameter, we will increase the number of parameters that we consider one at a time, from top to bottom in that list. In this way we can determine the optimal number of parameters for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize selected features\n",
    "for feature in selected_round1:\n",
    "    df[feature] = (df[feature] - df[feature].mean())/df[feature].std()\n",
    "\n",
    "# find the best number of parameters\n",
    "fscore_by_features = []\n",
    "for i in range(1, len(results)):\n",
    "    selected = [x[0] for x in results[:i]]\n",
    "    X = df[selected].values\n",
    "    y = df[['positive']].values\n",
    "\n",
    "    nn = NearestNeighbors(metric='euclidean', algorithm='auto')\n",
    "    nn.fit(X)\n",
    "    distances, indices = nn.kneighbors(X)\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i in range(len(X)):\n",
    "        nbrs_diseased = y[indices[i]].flatten()\n",
    "        predict = pd.Series(nbrs_diseased).mode()[0]  # Most common label\n",
    "        y_pred.append(predict)\n",
    "        y_true.append(y[i][0])\n",
    "\n",
    "    (p, r, f, s) = precision_recall_fscore_support(y_true, y_pred, labels=[1])\n",
    "    fscore_by_features.append(f)\n",
    "\n",
    "plt.plot(range(1, len(results)), fscore_by_features)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('F1 Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see a peak at 7 parameters, so we will use the first 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_round2 = [\n",
    "    'concave_points3',\n",
    "    'perimeter3',\n",
    "    'concave_points1',\n",
    "    'radius3',\n",
    "    'perimeter1',\n",
    "    'area3',\n",
    "    'radius1'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize for K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "X = df[selected_round2].values\n",
    "y = df[['positive']].values\n",
    "\n",
    "\n",
    "# Define the range of k values to try\n",
    "k_values = range(1, 201)  # Test k from 1 to 120\n",
    "results = predict_and_evaluate(X, y, k_values)\n",
    "\n",
    "plt.plot([x[0] for x in results], [x[3] for x in results])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('F1 Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we see an early peak, and then a linear dropoff. It seems like the peak stays around .93 from k=3 to k=25, but there is much more variance in the early k values, so we will select k=25 in the hope that the ouptput will be more consistient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
